{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#default_exp core\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covid19.data\n",
    "\n",
    "This is a module that understands how to parse the new covid19 data schema from the [Johns Hopkins CSSE project](https://github.com/CSSEGISandData/COVID-19). As of 3/23, the schema of the dataset changed, and the overall time series data only reports aggregated country statistics. The daily data remains, but needs to be parsed into a format where we can track trends on a state-by-state basis in the US and a province-by-province basis in Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIRMED_URI = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
    "DEATHS_URI = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the aggregated data, which is __mostly__ a country by country dataset. However, it is not consistent as there are provinces for Canada, but no individual states for the USA. Furthermore there are strange fields like \"Diamond Princess\" which presumably is just the data for that cruise ship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1/22/20</th>\n",
       "      <th>1/23/20</th>\n",
       "      <th>1/24/20</th>\n",
       "      <th>1/25/20</th>\n",
       "      <th>1/26/20</th>\n",
       "      <th>1/27/20</th>\n",
       "      <th>...</th>\n",
       "      <th>3/16/20</th>\n",
       "      <th>3/17/20</th>\n",
       "      <th>3/18/20</th>\n",
       "      <th>3/19/20</th>\n",
       "      <th>3/20/20</th>\n",
       "      <th>3/21/20</th>\n",
       "      <th>3/22/20</th>\n",
       "      <th>3/23/20</th>\n",
       "      <th>3/24/20</th>\n",
       "      <th>3/25/20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>65.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>74</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Albania</td>\n",
       "      <td>41.1533</td>\n",
       "      <td>20.1683</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>76</td>\n",
       "      <td>89</td>\n",
       "      <td>104</td>\n",
       "      <td>123</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>28.0339</td>\n",
       "      <td>1.6596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>60</td>\n",
       "      <td>74</td>\n",
       "      <td>87</td>\n",
       "      <td>90</td>\n",
       "      <td>139</td>\n",
       "      <td>201</td>\n",
       "      <td>230</td>\n",
       "      <td>264</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>42.5063</td>\n",
       "      <td>1.5218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>53</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>113</td>\n",
       "      <td>133</td>\n",
       "      <td>164</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-11.2027</td>\n",
       "      <td>17.8739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>17.0608</td>\n",
       "      <td>-61.7964</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>-38.4161</td>\n",
       "      <td>-63.6167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>68</td>\n",
       "      <td>79</td>\n",
       "      <td>97</td>\n",
       "      <td>128</td>\n",
       "      <td>158</td>\n",
       "      <td>266</td>\n",
       "      <td>301</td>\n",
       "      <td>387</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>40.0691</td>\n",
       "      <td>45.0382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>115</td>\n",
       "      <td>136</td>\n",
       "      <td>160</td>\n",
       "      <td>194</td>\n",
       "      <td>235</td>\n",
       "      <td>249</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Australian Capital Territory</td>\n",
       "      <td>Australia</td>\n",
       "      <td>-35.4735</td>\n",
       "      <td>149.0124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>-33.8688</td>\n",
       "      <td>151.2093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>171</td>\n",
       "      <td>210</td>\n",
       "      <td>267</td>\n",
       "      <td>307</td>\n",
       "      <td>353</td>\n",
       "      <td>436</td>\n",
       "      <td>669</td>\n",
       "      <td>669</td>\n",
       "      <td>818</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Province/State       Country/Region      Lat      Long  \\\n",
       "0                           NaN          Afghanistan  33.0000   65.0000   \n",
       "1                           NaN              Albania  41.1533   20.1683   \n",
       "2                           NaN              Algeria  28.0339    1.6596   \n",
       "3                           NaN              Andorra  42.5063    1.5218   \n",
       "4                           NaN               Angola -11.2027   17.8739   \n",
       "5                           NaN  Antigua and Barbuda  17.0608  -61.7964   \n",
       "6                           NaN            Argentina -38.4161  -63.6167   \n",
       "7                           NaN              Armenia  40.0691   45.0382   \n",
       "8  Australian Capital Territory            Australia -35.4735  149.0124   \n",
       "9               New South Wales            Australia -33.8688  151.2093   \n",
       "\n",
       "   1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  1/27/20  ...  3/16/20  \\\n",
       "0        0        0        0        0        0        0  ...       21   \n",
       "1        0        0        0        0        0        0  ...       51   \n",
       "2        0        0        0        0        0        0  ...       54   \n",
       "3        0        0        0        0        0        0  ...        2   \n",
       "4        0        0        0        0        0        0  ...        0   \n",
       "5        0        0        0        0        0        0  ...        1   \n",
       "6        0        0        0        0        0        0  ...       56   \n",
       "7        0        0        0        0        0        0  ...       52   \n",
       "8        0        0        0        0        0        0  ...        2   \n",
       "9        0        0        0        0        3        4  ...      171   \n",
       "\n",
       "   3/17/20  3/18/20  3/19/20  3/20/20  3/21/20  3/22/20  3/23/20  3/24/20  \\\n",
       "0       22       22       22       24       24       40       40       74   \n",
       "1       55       59       64       70       76       89      104      123   \n",
       "2       60       74       87       90      139      201      230      264   \n",
       "3       39       39       53       75       88      113      133      164   \n",
       "4        0        0        0        1        2        2        3        3   \n",
       "5        1        1        1        1        1        1        3        3   \n",
       "6       68       79       97      128      158      266      301      387   \n",
       "7       78       84      115      136      160      194      235      249   \n",
       "8        2        3        4        6        9       19       32       39   \n",
       "9      210      267      307      353      436      669      669      818   \n",
       "\n",
       "   3/25/20  \n",
       "0       84  \n",
       "1      146  \n",
       "2      302  \n",
       "3      188  \n",
       "4        3  \n",
       "5        3  \n",
       "6      387  \n",
       "7      265  \n",
       "8       39  \n",
       "9     1029  \n",
       "\n",
       "[10 rows x 68 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_df = pd.read_csv(CONFIRMED_URI)\n",
    "confirmed_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we examine the daily reports, the individual day's data is constructed based on a filename with a `MM-DD-YYYY.csv` format, e.g., `03-24-2020.csv`. Let's define a function that reads the daily reports for a specific day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import datetime as dt\n",
    "\n",
    "DAILY_REPORT_BASE_URI = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "\n",
    "def read_daily_report(date):\n",
    "    \"\"\"Read the daily report for date\"\"\"\n",
    "    assert(isinstance(date, dt.date)), \"parameter date must be a date object\"\n",
    "    uri = DAILY_REPORT_BASE_URI + date.strftime(\"%m-%d-%Y.csv\")\n",
    "    df = pd.read_csv(uri)\n",
    "    df = df.rename(columns={\"Province/State\": \"Province_State\", \"Country/Region\": \"Country_Region\"})\n",
    "    df.set_index(['Province_State', 'Country_Region'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily report has several fields that are interesting. Let's compute a dataframe that contains the data for all the `Province_State` records in the US. We begin by reading the daily report into a dataframe and looking at its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45001.0</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-24 23:37:31</td>\n",
       "      <td>34.223334</td>\n",
       "      <td>-82.461707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbeville, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22001.0</td>\n",
       "      <td>Acadia</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-24 23:37:31</td>\n",
       "      <td>30.295065</td>\n",
       "      <td>-92.414197</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Acadia, Louisiana, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51001.0</td>\n",
       "      <td>Accomack</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-24 23:37:31</td>\n",
       "      <td>37.767072</td>\n",
       "      <td>-75.632346</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Accomack, Virginia, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16001.0</td>\n",
       "      <td>Ada</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-24 23:37:31</td>\n",
       "      <td>43.452658</td>\n",
       "      <td>-116.241552</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ada, Idaho, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19001.0</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-24 23:37:31</td>\n",
       "      <td>41.330756</td>\n",
       "      <td>-94.471059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Adair, Iowa, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS     Admin2  Province_State Country_Region          Last_Update  \\\n",
       "0  45001.0  Abbeville  South Carolina             US  2020-03-24 23:37:31   \n",
       "1  22001.0     Acadia       Louisiana             US  2020-03-24 23:37:31   \n",
       "2  51001.0   Accomack        Virginia             US  2020-03-24 23:37:31   \n",
       "3  16001.0        Ada           Idaho             US  2020-03-24 23:37:31   \n",
       "4  19001.0      Adair            Iowa             US  2020-03-24 23:37:31   \n",
       "\n",
       "         Lat       Long_  Confirmed  Deaths  Recovered  Active  \\\n",
       "0  34.223334  -82.461707          1       0          0       0   \n",
       "1  30.295065  -92.414197          2       0          0       0   \n",
       "2  37.767072  -75.632346          1       0          0       0   \n",
       "3  43.452658 -116.241552         19       0          0       0   \n",
       "4  41.330756  -94.471059          1       0          0       0   \n",
       "\n",
       "                    Combined_Key  \n",
       "0  Abbeville, South Carolina, US  \n",
       "1          Acadia, Louisiana, US  \n",
       "2         Accomack, Virginia, US  \n",
       "3                 Ada, Idaho, US  \n",
       "4                Adair, Iowa, US  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_daily_report(dt.date(2020,3,24))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to do a pandas group by operation over `Province_State` for all `Country_Region == 'US'`. We will use the pandas [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Province_State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>71489.0</td>\n",
       "      <td>2203.246784</td>\n",
       "      <td>-5809.578199</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>62340.0</td>\n",
       "      <td>1747.579877</td>\n",
       "      <td>-4229.319334</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Samoa</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>-14.271000</td>\n",
       "      <td>-170.132000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>60208.0</td>\n",
       "      <td>505.138555</td>\n",
       "      <td>-1671.948482</td>\n",
       "      <td>326</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>380625.0</td>\n",
       "      <td>2618.391704</td>\n",
       "      <td>-6932.548370</td>\n",
       "      <td>219</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    FIPS          Lat        Long_  Confirmed  Deaths  \\\n",
       "Province_State                                                          \n",
       "Alabama          71489.0  2203.246784 -5809.578199        242       0   \n",
       "Alaska           62340.0  1747.579877 -4229.319334         34       0   \n",
       "American Samoa   60000.0   -14.271000  -170.132000          0       0   \n",
       "Arizona          60208.0   505.138555 -1671.948482        326       5   \n",
       "Arkansas        380625.0  2618.391704 -6932.548370        219       2   \n",
       "\n",
       "                Recovered  Active  \n",
       "Province_State                     \n",
       "Alabama                 0       0  \n",
       "Alaska                  0       0  \n",
       "American Samoa          0       0  \n",
       "Arizona                 0       0  \n",
       "Arkansas                0       0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_df = df.loc[df['Country_Region'] == 'US'].groupby(\"Province_State\").sum()\n",
    "states_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are regions in the US, not just the states so we have rows for American Samoa, Puerto Rico, District of Columbia among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is data for just a single point in time, i.e., 03-24-2020. We want the time series data, so we'll need to go back through time to compute this data. Let's create a function that will load a local dataframe that contains the cached results for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def read_historical_data(start_date):\n",
    "    \"\"\"Read all of the historical data starting at start_date\"\"\"\n",
    "    def daterange(start, end):\n",
    "        for day in range(int((end-start).days)+1):\n",
    "            yield start + dt.timedelta(day)\n",
    "    \n",
    "    result = []\n",
    "    for day in daterange(start_date, dt.date.today()):\n",
    "        try:\n",
    "            df = read_daily_report(day)\n",
    "            result.append((day, df))\n",
    "        except urllib.error.HTTPError as err:\n",
    "            # Today's data may not be available\n",
    "            if err.code == 404 and day == dt.date.today():\n",
    "                return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a dictionary from the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = read_historical_data(dt.date(2020,3,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the computation using just two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hubei</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>2020-03-08T14:43:03</td>\n",
       "      <td>67707</td>\n",
       "      <td>2986</td>\n",
       "      <td>45235</td>\n",
       "      <td>30.9756</td>\n",
       "      <td>112.2707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2020-03-08T18:03:04</td>\n",
       "      <td>7375</td>\n",
       "      <td>366</td>\n",
       "      <td>622</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>2020-03-08T12:53:03</td>\n",
       "      <td>7314</td>\n",
       "      <td>50</td>\n",
       "      <td>118</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>128.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iran</td>\n",
       "      <td>2020-03-08T11:03:30</td>\n",
       "      <td>6566</td>\n",
       "      <td>194</td>\n",
       "      <td>2134</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>53.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guangdong</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>2020-03-08T14:43:03</td>\n",
       "      <td>1352</td>\n",
       "      <td>7</td>\n",
       "      <td>1256</td>\n",
       "      <td>23.3417</td>\n",
       "      <td>113.4244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province_State  Country_Region          Last Update  Confirmed  Deaths  \\\n",
       "0          Hubei  Mainland China  2020-03-08T14:43:03      67707    2986   \n",
       "1            NaN           Italy  2020-03-08T18:03:04       7375     366   \n",
       "2            NaN     South Korea  2020-03-08T12:53:03       7314      50   \n",
       "3            NaN            Iran  2020-03-08T11:03:30       6566     194   \n",
       "4      Guangdong  Mainland China  2020-03-08T14:43:03       1352       7   \n",
       "\n",
       "   Recovered  Latitude  Longitude  \n",
       "0      45235   30.9756   112.2707  \n",
       "1        622   43.0000    12.0000  \n",
       "2        118   36.0000   128.0000  \n",
       "3       2134   32.0000    53.0000  \n",
       "4       1256   23.3417   113.4244  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, df2 = historical_data[1]\n",
    "print(d)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45001.0</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>34.223334</td>\n",
       "      <td>-82.461707</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbeville, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45003.0</td>\n",
       "      <td>Aiken</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>33.543380</td>\n",
       "      <td>-81.636454</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Aiken, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>45005.0</td>\n",
       "      <td>Allendale</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>32.988374</td>\n",
       "      <td>-81.353211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Allendale, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>45007.0</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>34.518281</td>\n",
       "      <td>-82.639595</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Anderson, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>45009.0</td>\n",
       "      <td>Bamberg</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>33.219276</td>\n",
       "      <td>-81.056600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bamberg, South Carolina, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FIPS     Admin2  Province_State Country_Region          Last_Update  \\\n",
       "0    45001.0  Abbeville  South Carolina             US  2020-03-25 23:33:19   \n",
       "21   45003.0      Aiken  South Carolina             US  2020-03-25 23:33:19   \n",
       "51   45005.0  Allendale  South Carolina             US  2020-03-25 23:33:19   \n",
       "61   45007.0   Anderson  South Carolina             US  2020-03-25 23:33:19   \n",
       "124  45009.0    Bamberg  South Carolina             US  2020-03-25 23:33:19   \n",
       "\n",
       "           Lat      Long_  Confirmed  Deaths  Recovered  Active  \\\n",
       "0    34.223334 -82.461707          3       0          0       0   \n",
       "21   33.543380 -81.636454          2       0          0       0   \n",
       "51   32.988374 -81.353211          0       0          0       0   \n",
       "61   34.518281 -82.639595         19       0          0       0   \n",
       "124  33.219276 -81.056600          0       0          0       0   \n",
       "\n",
       "                      Combined_Key  \n",
       "0    Abbeville, South Carolina, US  \n",
       "21       Aiken, South Carolina, US  \n",
       "51   Allendale, South Carolina, US  \n",
       "61    Anderson, South Carolina, US  \n",
       "124    Bamberg, South Carolina, US  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, df1 = historical_data[len(historical_data)-1]\n",
    "print(d)\n",
    "df1.loc[(df1[\"Province_State\"]==\"South Carolina\") & (df1[\"Country_Region\"]==\"US\")].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the schema changed on `03-23-2020` from:\n",
    "\n",
    "`Province/State, Country/Region, Last Update, Confirmed, Deaths, Recovered, Latitude, Longitude`\n",
    "\n",
    "to:\n",
    "\n",
    "`FIPS, Admin2, Province_State, Country_Region, Lat, Long_, Confirmed, Deaths, Recovered, Active, Combined_Key`\n",
    "\n",
    "We will use the latter schema and convert data from the older schema to the newer schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>2020-03-25</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45001.0</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>34.223334</td>\n",
       "      <td>-82.461707</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbeville, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22001.0</td>\n",
       "      <td>Acadia</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>30.295065</td>\n",
       "      <td>-92.414197</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Acadia, Louisiana, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51001.0</td>\n",
       "      <td>Accomack</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>37.767072</td>\n",
       "      <td>-75.632346</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Accomack, Virginia, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16001.0</td>\n",
       "      <td>Ada</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>43.452658</td>\n",
       "      <td>-116.241552</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ada, Idaho, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19001.0</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>41.330756</td>\n",
       "      <td>-94.471059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Adair, Iowa, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS     Admin2  Province_State Country_Region          Last_Update  \\\n",
       "0  45001.0  Abbeville  South Carolina             US  2020-03-25 23:33:19   \n",
       "1  22001.0     Acadia       Louisiana             US  2020-03-25 23:33:19   \n",
       "2  51001.0   Accomack        Virginia             US  2020-03-25 23:33:19   \n",
       "3  16001.0        Ada           Idaho             US  2020-03-25 23:33:19   \n",
       "4  19001.0      Adair            Iowa             US  2020-03-25 23:33:19   \n",
       "\n",
       "         Lat       Long_  2020-03-25  Deaths  Recovered  Active  \\\n",
       "0  34.223334  -82.461707           3       0          0       0   \n",
       "1  30.295065  -92.414197           2       0          0       0   \n",
       "2  37.767072  -75.632346           2       0          0       0   \n",
       "3  43.452658 -116.241552          24       0          0       0   \n",
       "4  41.330756  -94.471059           1       0          0       0   \n",
       "\n",
       "                    Combined_Key  \n",
       "0  Abbeville, South Carolina, US  \n",
       "1          Acadia, Louisiana, US  \n",
       "2         Accomack, Virginia, US  \n",
       "3                 Ada, Idaho, US  \n",
       "4                Adair, Iowa, US  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.rename(columns={\"Confirmed\": dt.date(2020,3,25)})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no state-wide aggregate data in earlier reports. So we have a couple of different options here:\n",
    "\n",
    "1. We can create a manual mapping \n",
    "1. We can create an autoamtic mapping based on some kind of regex (but even a cursory inspection says this will be hard, i.e., \"King County\" vs. \"King\"\n",
    "1. We can create a state-wide mapping again based on some kind of mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>King County, WA</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T20:23:09</td>\n",
       "      <td>83</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>47.5480</td>\n",
       "      <td>-121.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Westchester County, NY</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T18:03:07</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.1220</td>\n",
       "      <td>-73.7949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Unassigned Location (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-02T19:53:03</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4437</td>\n",
       "      <td>139.6380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Santa Clara County, CA</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T21:23:03</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.3541</td>\n",
       "      <td>-121.9552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Snohomish County, WA</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T21:43:03</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0330</td>\n",
       "      <td>-121.8339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Yolo County, CA</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-06T20:13:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7646</td>\n",
       "      <td>-121.9018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Lackland, TX (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.3829</td>\n",
       "      <td>-98.6134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Montgomery County, TX</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T19:53:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3213</td>\n",
       "      <td>-95.4778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Omaha, NE (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.2545</td>\n",
       "      <td>-95.9758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Travis, CA (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.2721</td>\n",
       "      <td>-121.9399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Province_State Country_Region  \\\n",
       "45                               King County, WA             US   \n",
       "46                        Westchester County, NY             US   \n",
       "57   Unassigned Location (From Diamond Princess)             US   \n",
       "62                        Santa Clara County, CA             US   \n",
       "67                          Snohomish County, WA             US   \n",
       "..                                           ...            ...   \n",
       "246                              Yolo County, CA             US   \n",
       "251         Lackland, TX (From Diamond Princess)             US   \n",
       "252                        Montgomery County, TX             US   \n",
       "253            Omaha, NE (From Diamond Princess)             US   \n",
       "254           Travis, CA (From Diamond Princess)             US   \n",
       "\n",
       "             Last Update  Confirmed  Deaths  Recovered  Latitude  Longitude  \n",
       "45   2020-03-08T20:23:09         83      17          1   47.5480  -121.9836  \n",
       "46   2020-03-08T18:03:07         83       0          0   41.1220   -73.7949  \n",
       "57   2020-03-02T19:53:03         45       0          0   35.4437   139.6380  \n",
       "62   2020-03-08T21:23:03         38       0          1   37.3541  -121.9552  \n",
       "67   2020-03-08T21:43:03         31       1          0   48.0330  -121.8339  \n",
       "..                   ...        ...     ...        ...       ...        ...  \n",
       "246  2020-03-06T20:13:14          1       0          0   38.7646  -121.9018  \n",
       "251  2020-02-24T23:33:02          0       0          0   29.3829   -98.6134  \n",
       "252  2020-03-07T19:53:02          0       0          0   30.3213   -95.4778  \n",
       "253  2020-02-24T23:33:02          0       0          0   41.2545   -95.9758  \n",
       "254  2020-02-24T23:33:02          0       0          0   38.2721  -121.9399  \n",
       "\n",
       "[106 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[(df2[\"Country_Region\"]==\"US\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple regex against `Province_State` to see what that reveals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King WA\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "str = \"King County, WA\"\n",
    "res = re.match(\"(?P<county>.*?)\\s+County,\\s+(?P<state>.*)\", str)\n",
    "county, state = res.group(1), res.group(2)\n",
    "print(county, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a table of state abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abbreviation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   State\n",
       "Abbreviation            \n",
       "AL               Alabama\n",
       "AK                Alaska\n",
       "AZ               Arizona\n",
       "AR              Arkansas\n",
       "CA            California"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_abbreviations = pd.read_csv(\"./data/state_abbreviations.csv\", index_col=\"Abbreviation\")\n",
    "state_abbreviations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's lookup a state based on its abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_abbreviations.loc['NY'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting closer, but I really want to apply a lambda function across each row in this result set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_state(province_state):\n",
    "    if not isinstance(province_state, type(str)):\n",
    "        return \"\"\n",
    "    res = re.match(\"(.*?)\\s+County,\\s+(\\S*)\", province_state)\n",
    "    if res == None:\n",
    "        return province_state\n",
    "    county, state = res.group(1), res.group(2)\n",
    "    result = state_abbreviations.loc[state].iloc[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_county(province_state):\n",
    "    if not isinstance(province_state, type(str)):\n",
    "        return \"\"\n",
    "    res = re.match(\"(.*?)\\s+County,\\s+(\\S*)\", province_state)\n",
    "    if res == None:\n",
    "        return \"\"\n",
    "    return res.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     Hubei\n",
       "1                                       NaN\n",
       "2                                       NaN\n",
       "3                                       NaN\n",
       "4                                 Guangdong\n",
       "                       ...                 \n",
       "250                      Northern Territory\n",
       "251    Lackland, TX (From Diamond Princess)\n",
       "252                   Montgomery County, TX\n",
       "253       Omaha, NE (From Diamond Princess)\n",
       "254      Travis, CA (From Diamond Princess)\n",
       "Name: Province_State, Length: 255, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"Province_State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Admin</th>\n",
       "      <th>Province_State2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T20:23:09</td>\n",
       "      <td>83</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>47.5480</td>\n",
       "      <td>-121.9836</td>\n",
       "      <td>King</td>\n",
       "      <td>King</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T18:03:07</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.1220</td>\n",
       "      <td>-73.7949</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Unassigned Location (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-02T19:53:03</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4437</td>\n",
       "      <td>139.6380</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Unassigned Location (From Diamond Princess)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>California</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T21:23:03</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.3541</td>\n",
       "      <td>-121.9552</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-08T21:43:03</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0330</td>\n",
       "      <td>-121.8339</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>California</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-06T20:13:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7646</td>\n",
       "      <td>-121.9018</td>\n",
       "      <td>Yolo</td>\n",
       "      <td>Yolo</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Lackland, TX (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.3829</td>\n",
       "      <td>-98.6134</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Lackland, TX (From Diamond Princess)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T19:53:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3213</td>\n",
       "      <td>-95.4778</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Omaha, NE (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.2545</td>\n",
       "      <td>-95.9758</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Omaha, NE (From Diamond Princess)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Travis, CA (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.2721</td>\n",
       "      <td>-121.9399</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Travis, CA (From Diamond Princess)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Province_State Country_Region  \\\n",
       "45                                    Washington             US   \n",
       "46                                      New York             US   \n",
       "57   Unassigned Location (From Diamond Princess)             US   \n",
       "62                                    California             US   \n",
       "67                                    Washington             US   \n",
       "..                                           ...            ...   \n",
       "246                                   California             US   \n",
       "251         Lackland, TX (From Diamond Princess)             US   \n",
       "252                                        Texas             US   \n",
       "253            Omaha, NE (From Diamond Princess)             US   \n",
       "254           Travis, CA (From Diamond Princess)             US   \n",
       "\n",
       "             Last Update  Confirmed  Deaths  Recovered  Latitude  Longitude  \\\n",
       "45   2020-03-08T20:23:09         83      17          1   47.5480  -121.9836   \n",
       "46   2020-03-08T18:03:07         83       0          0   41.1220   -73.7949   \n",
       "57   2020-03-02T19:53:03         45       0          0   35.4437   139.6380   \n",
       "62   2020-03-08T21:23:03         38       0          1   37.3541  -121.9552   \n",
       "67   2020-03-08T21:43:03         31       1          0   48.0330  -121.8339   \n",
       "..                   ...        ...     ...        ...       ...        ...   \n",
       "246  2020-03-06T20:13:14          1       0          0   38.7646  -121.9018   \n",
       "251  2020-02-24T23:33:02          0       0          0   29.3829   -98.6134   \n",
       "252  2020-03-07T19:53:02          0       0          0   30.3213   -95.4778   \n",
       "253  2020-02-24T23:33:02          0       0          0   41.2545   -95.9758   \n",
       "254  2020-02-24T23:33:02          0       0          0   38.2721  -121.9399   \n",
       "\n",
       "          Admin2        Admin                              Province_State2  \n",
       "45          King         King                                   Washington  \n",
       "46   Westchester  Westchester                                     New York  \n",
       "57                             Unassigned Location (From Diamond Princess)  \n",
       "62   Santa Clara  Santa Clara                                   California  \n",
       "67     Snohomish    Snohomish                                   Washington  \n",
       "..           ...          ...                                          ...  \n",
       "246         Yolo         Yolo                                   California  \n",
       "251                                   Lackland, TX (From Diamond Princess)  \n",
       "252   Montgomery   Montgomery                                        Texas  \n",
       "253                                      Omaha, NE (From Diamond Princess)  \n",
       "254                                     Travis, CA (From Diamond Princess)  \n",
       "\n",
       "[106 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"Admin2\"] = df2[\"Province_State\"].apply(extract_county)\n",
    "df2[\"Province_State\"] = df2[\"Province_State\"].apply(extract_state)\n",
    "df2.loc[df2[\"Country_Region\"]==\"US\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update the `read_daily_report` function to perform transform on old schema files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "DAILY_REPORT_BASE_URI = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "\n",
    "def read_daily_report(date):\n",
    "    \"\"\"Read the daily report for date\"\"\"\n",
    "    assert(isinstance(date, dt.date)), \"parameter date must be a date object\"\n",
    "    uri = DAILY_REPORT_BASE_URI + date.strftime(\"%m-%d-%Y.csv\")\n",
    "    df = pd.read_csv(uri)\n",
    "    \n",
    "    # Adjust for old schema\n",
    "    if date < dt.date(2020, 3, 22):\n",
    "        df = df.rename(columns={\"Province/State\": \"Province_State\", \"Country/Region\": \"Country_Region\"})\n",
    "        df[\"Admin2\"] = df[\"Province_State\"].apply(extract_county)\n",
    "        df[\"Province_State\"] = df[\"Province_State\"].apply(extract_state)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = read_historical_data(dt.date(2020,3,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Admin2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T20:23:03</td>\n",
       "      <td>71</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>47.5480</td>\n",
       "      <td>-121.9836</td>\n",
       "      <td>King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T18:23:05</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.1220</td>\n",
       "      <td>-73.7949</td>\n",
       "      <td>Westchester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Unassigned Location (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-02T19:53:03</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4437</td>\n",
       "      <td>139.6380</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>California</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T23:43:03</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.3541</td>\n",
       "      <td>-121.9552</td>\n",
       "      <td>Santa Clara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T20:23:03</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0330</td>\n",
       "      <td>-121.8339</td>\n",
       "      <td>Snohomish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>California</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-06T20:13:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7646</td>\n",
       "      <td>-121.9018</td>\n",
       "      <td>Yolo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Lackland, TX (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.3829</td>\n",
       "      <td>-98.6134</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-07T19:53:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.3213</td>\n",
       "      <td>-95.4778</td>\n",
       "      <td>Montgomery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Omaha, NE (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.2545</td>\n",
       "      <td>-95.9758</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Travis, CA (From Diamond Princess)</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-02-24T23:33:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.2721</td>\n",
       "      <td>-121.9399</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Province_State Country_Region  \\\n",
       "48                                    Washington             US   \n",
       "50                                      New York             US   \n",
       "56   Unassigned Location (From Diamond Princess)             US   \n",
       "59                                    California             US   \n",
       "62                                    Washington             US   \n",
       "..                                           ...            ...   \n",
       "216                                   California             US   \n",
       "221         Lackland, TX (From Diamond Princess)             US   \n",
       "222                                        Texas             US   \n",
       "223            Omaha, NE (From Diamond Princess)             US   \n",
       "224           Travis, CA (From Diamond Princess)             US   \n",
       "\n",
       "             Last Update  Confirmed  Deaths  Recovered  Latitude  Longitude  \\\n",
       "48   2020-03-07T20:23:03         71      15          1   47.5480  -121.9836   \n",
       "50   2020-03-07T18:23:05         57       0          0   41.1220   -73.7949   \n",
       "56   2020-03-02T19:53:03         45       0          0   35.4437   139.6380   \n",
       "59   2020-03-07T23:43:03         32       0          1   37.3541  -121.9552   \n",
       "62   2020-03-07T20:23:03         27       1          0   48.0330  -121.8339   \n",
       "..                   ...        ...     ...        ...       ...        ...   \n",
       "216  2020-03-06T20:13:14          1       0          0   38.7646  -121.9018   \n",
       "221  2020-02-24T23:33:02          0       0          0   29.3829   -98.6134   \n",
       "222  2020-03-07T19:53:02          0       0          0   30.3213   -95.4778   \n",
       "223  2020-02-24T23:33:02          0       0          0   41.2545   -95.9758   \n",
       "224  2020-02-24T23:33:02          0       0          0   38.2721  -121.9399   \n",
       "\n",
       "          Admin2  \n",
       "48          King  \n",
       "50   Westchester  \n",
       "56                \n",
       "59   Santa Clara  \n",
       "62     Snohomish  \n",
       "..           ...  \n",
       "216         Yolo  \n",
       "221               \n",
       "222   Montgomery  \n",
       "223               \n",
       "224               \n",
       "\n",
       "[82 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, df = historical_data[0]\n",
    "df.loc[df[\"Country_Region\"]==\"US\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take an old schema dataframe `df2` and a new schema dataframe `df1` and join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>2020-03-25</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45001.0</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>34.223334</td>\n",
       "      <td>-82.461707</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbeville, South Carolina, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22001.0</td>\n",
       "      <td>Acadia</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>30.295065</td>\n",
       "      <td>-92.414197</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Acadia, Louisiana, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51001.0</td>\n",
       "      <td>Accomack</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>37.767072</td>\n",
       "      <td>-75.632346</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Accomack, Virginia, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16001.0</td>\n",
       "      <td>Ada</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>43.452658</td>\n",
       "      <td>-116.241552</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ada, Idaho, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19001.0</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-03-25 23:33:19</td>\n",
       "      <td>41.330756</td>\n",
       "      <td>-94.471059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Adair, Iowa, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS     Admin2  Province_State Country_Region          Last_Update  \\\n",
       "0  45001.0  Abbeville  South Carolina             US  2020-03-25 23:33:19   \n",
       "1  22001.0     Acadia       Louisiana             US  2020-03-25 23:33:19   \n",
       "2  51001.0   Accomack        Virginia             US  2020-03-25 23:33:19   \n",
       "3  16001.0        Ada           Idaho             US  2020-03-25 23:33:19   \n",
       "4  19001.0      Adair            Iowa             US  2020-03-25 23:33:19   \n",
       "\n",
       "         Lat       Long_  2020-03-25  Deaths  Recovered  Active  \\\n",
       "0  34.223334  -82.461707           3       0          0       0   \n",
       "1  30.295065  -92.414197           2       0          0       0   \n",
       "2  37.767072  -75.632346           2       0          0       0   \n",
       "3  43.452658 -116.241552          24       0          0       0   \n",
       "4  41.330756  -94.471059           1       0          0       0   \n",
       "\n",
       "                    Combined_Key  \n",
       "0  Abbeville, South Carolina, US  \n",
       "1          Acadia, Louisiana, US  \n",
       "2         Accomack, Virginia, US  \n",
       "3                 Ada, Idaho, US  \n",
       "4                Adair, Iowa, US  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.rename(columns={\"Confirmed\": dt.date(2020,3,25)})\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Admin</th>\n",
       "      <th>Province_State2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hubei</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>2020-03-08T14:43:03</td>\n",
       "      <td>67707</td>\n",
       "      <td>2986</td>\n",
       "      <td>45235</td>\n",
       "      <td>30.9756</td>\n",
       "      <td>112.2707</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Hubei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Italy</td>\n",
       "      <td>2020-03-08T18:03:04</td>\n",
       "      <td>7375</td>\n",
       "      <td>366</td>\n",
       "      <td>622</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>South Korea</td>\n",
       "      <td>2020-03-08T12:53:03</td>\n",
       "      <td>7314</td>\n",
       "      <td>50</td>\n",
       "      <td>118</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>128.0000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Iran</td>\n",
       "      <td>2020-03-08T11:03:30</td>\n",
       "      <td>6566</td>\n",
       "      <td>194</td>\n",
       "      <td>2134</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>53.0000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guangdong</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>2020-03-08T14:43:03</td>\n",
       "      <td>1352</td>\n",
       "      <td>7</td>\n",
       "      <td>1256</td>\n",
       "      <td>23.3417</td>\n",
       "      <td>113.4244</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Guangdong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province_State  Country_Region          Last Update  Confirmed  Deaths  \\\n",
       "0          Hubei  Mainland China  2020-03-08T14:43:03      67707    2986   \n",
       "1                          Italy  2020-03-08T18:03:04       7375     366   \n",
       "2                    South Korea  2020-03-08T12:53:03       7314      50   \n",
       "3                           Iran  2020-03-08T11:03:30       6566     194   \n",
       "4      Guangdong  Mainland China  2020-03-08T14:43:03       1352       7   \n",
       "\n",
       "   Recovered  Latitude  Longitude Admin2 Admin Province_State2  \n",
       "0      45235   30.9756   112.2707                        Hubei  \n",
       "1        622   43.0000    12.0000                               \n",
       "2        118   36.0000   128.0000                               \n",
       "3       2134   32.0000    53.0000                               \n",
       "4       1256   23.3417   113.4244                    Guangdong  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.rename(columns={dt.date(2020,3,8): 'Confirmed'})\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to join `df1` and `df2` using `Admin2`, `Province_State`, and `Country_Region`, extract the `Confirmed` column and append it as a new column with the date of that dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3420 255\n"
     ]
    }
   ],
   "source": [
    "print(len(df1), len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>2020-03-25</th>\n",
       "      <th>Confirmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "      <td>34.223334</td>\n",
       "      <td>-82.461707</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22001</td>\n",
       "      <td>Acadia</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>US</td>\n",
       "      <td>30.295065</td>\n",
       "      <td>-92.414197</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51001</td>\n",
       "      <td>Accomack</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>US</td>\n",
       "      <td>37.767072</td>\n",
       "      <td>-75.632346</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16001</td>\n",
       "      <td>Ada</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>US</td>\n",
       "      <td>43.452658</td>\n",
       "      <td>-116.241552</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19001</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>US</td>\n",
       "      <td>41.330756</td>\n",
       "      <td>-94.471059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21001</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>US</td>\n",
       "      <td>37.104598</td>\n",
       "      <td>-85.281297</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29001</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>US</td>\n",
       "      <td>40.190586</td>\n",
       "      <td>-92.600782</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40001</td>\n",
       "      <td>Adair</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>US</td>\n",
       "      <td>35.884942</td>\n",
       "      <td>-94.658593</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8001</td>\n",
       "      <td>Adams</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>US</td>\n",
       "      <td>39.874321</td>\n",
       "      <td>-104.336258</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16003</td>\n",
       "      <td>Adams</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>US</td>\n",
       "      <td>44.893336</td>\n",
       "      <td>-116.454525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FIPS     Admin2  Province_State Country_Region        Lat       Long_  \\\n",
       "0  45001  Abbeville  South Carolina             US  34.223334  -82.461707   \n",
       "1  22001     Acadia       Louisiana             US  30.295065  -92.414197   \n",
       "2  51001   Accomack        Virginia             US  37.767072  -75.632346   \n",
       "3  16001        Ada           Idaho             US  43.452658 -116.241552   \n",
       "4  19001      Adair            Iowa             US  41.330756  -94.471059   \n",
       "5  21001      Adair        Kentucky             US  37.104598  -85.281297   \n",
       "6  29001      Adair        Missouri             US  40.190586  -92.600782   \n",
       "7  40001      Adair        Oklahoma             US  35.884942  -94.658593   \n",
       "8   8001      Adams        Colorado             US  39.874321 -104.336258   \n",
       "9  16003      Adams           Idaho             US  44.893336 -116.454525   \n",
       "\n",
       "   2020-03-25  Confirmed  \n",
       "0           3          0  \n",
       "1           2          0  \n",
       "2           2          0  \n",
       "3          24          0  \n",
       "4           1          0  \n",
       "5           0          0  \n",
       "6           1          0  \n",
       "7           2          0  \n",
       "8          27          0  \n",
       "9           0          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", dt.date(2020,3,25)]].merge(df2[[\"Admin2\", \"Province_State\", \"Country_Region\", \"Confirmed\"]], on=[\"Admin2\", \"Province_State\", \"Country_Region\"], how=\"outer\").fillna(0, downcast='infer')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that will do the merge from most recent to oldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized = False\n",
    "for d, df in reversed(historical_data):\n",
    "    if not initialized:\n",
    "        cdf = df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Confirmed\"]].fillna(value={\"Admin2\": \"\", \"Province_State\": \"\"}, downcast=\"infer\")\n",
    "        ddf = df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Deaths\"]].fillna(value={\"Admin2\": \"\", \"Province_State\": \"\"}, downcast=\"infer\")\n",
    "        initialized = True\n",
    "    else:\n",
    "        df = df.fillna(value={\"Admin2\": \"\", \"Province_State\": \"\"}, downcast=\"infer\")\n",
    "        cdf = cdf.merge(df[[\"Admin2\", \"Province_State\", \"Country_Region\", \"Confirmed\"]], on=[\"Admin2\", \"Province_State\", \"Country_Region\"], how=\"outer\")\n",
    "        ddf = ddf.merge(df[[\"Admin2\", \"Province_State\", \"Country_Region\", \"Deaths\"]], on=[\"Admin2\", \"Province_State\", \"Country_Region\"], how=\"outer\")\n",
    "\n",
    "    # Rename the merged columns\n",
    "    cdf = cdf.rename(columns={\"Confirmed\": d})\n",
    "    ddf = ddf.rename(columns={\"Deaths\": d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>2020-03-26</th>\n",
       "      <th>2020-03-25</th>\n",
       "      <th>2020-03-24</th>\n",
       "      <th>2020-03-23</th>\n",
       "      <th>...</th>\n",
       "      <th>2020-03-16</th>\n",
       "      <th>2020-03-15</th>\n",
       "      <th>2020-03-14</th>\n",
       "      <th>2020-03-13</th>\n",
       "      <th>2020-03-12</th>\n",
       "      <th>2020-03-11</th>\n",
       "      <th>2020-03-10</th>\n",
       "      <th>2020-03-09</th>\n",
       "      <th>2020-03-08</th>\n",
       "      <th>2020-03-07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Canada</td>\n",
       "      <td>53.9333</td>\n",
       "      <td>-116.5765</td>\n",
       "      <td>486.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Canada</td>\n",
       "      <td>53.7267</td>\n",
       "      <td>-127.6476</td>\n",
       "      <td>725.0</td>\n",
       "      <td>617.0</td>\n",
       "      <td>617.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>...</td>\n",
       "      <td>103.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Diamond Princess</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Grand Princess</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Manitoba</td>\n",
       "      <td>Canada</td>\n",
       "      <td>53.7609</td>\n",
       "      <td>-98.8139</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>New Brunswick</td>\n",
       "      <td>Canada</td>\n",
       "      <td>46.5653</td>\n",
       "      <td>-66.4619</td>\n",
       "      <td>33.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Newfoundland and Labrador</td>\n",
       "      <td>Canada</td>\n",
       "      <td>53.1355</td>\n",
       "      <td>-57.6604</td>\n",
       "      <td>82.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Canada</td>\n",
       "      <td>64.8255</td>\n",
       "      <td>-124.8457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Nova Scotia</td>\n",
       "      <td>Canada</td>\n",
       "      <td>44.6820</td>\n",
       "      <td>-63.7443</td>\n",
       "      <td>73.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Canada</td>\n",
       "      <td>51.2538</td>\n",
       "      <td>-85.3232</td>\n",
       "      <td>858.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>...</td>\n",
       "      <td>177.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Prince Edward Island</td>\n",
       "      <td>Canada</td>\n",
       "      <td>46.5107</td>\n",
       "      <td>-63.4168</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Quebec</td>\n",
       "      <td>Canada</td>\n",
       "      <td>52.9399</td>\n",
       "      <td>-73.5491</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>1342.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Recovered</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3232</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Saskatchewan</td>\n",
       "      <td>Canada</td>\n",
       "      <td>52.9399</td>\n",
       "      <td>-106.4509</td>\n",
       "      <td>95.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Yukon</td>\n",
       "      <td>Canada</td>\n",
       "      <td>64.2823</td>\n",
       "      <td>-135.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Montreal, QC</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Edmonton, Alberta</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Calgary, Alberta</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>London, ON</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FIPS Admin2             Province_State Country_Region      Lat  \\\n",
       "3167   NaN                           Alberta         Canada  53.9333   \n",
       "3174   NaN                  British Columbia         Canada  53.7267   \n",
       "3179   NaN                  Diamond Princess         Canada   0.0000   \n",
       "3187   NaN                    Grand Princess         Canada   0.0000   \n",
       "3209   NaN                          Manitoba         Canada  53.7609   \n",
       "3213   NaN                     New Brunswick         Canada  46.5653   \n",
       "3216   NaN         Newfoundland and Labrador         Canada  53.1355   \n",
       "3220   NaN             Northwest Territories         Canada  64.8255   \n",
       "3221   NaN                       Nova Scotia         Canada  44.6820   \n",
       "3222   NaN                           Ontario         Canada  51.2538   \n",
       "3223   NaN              Prince Edward Island         Canada  46.5107   \n",
       "3226   NaN                            Quebec         Canada  52.9399   \n",
       "3228   NaN                         Recovered         Canada   0.0000   \n",
       "3232   NaN                      Saskatchewan         Canada  52.9399   \n",
       "3248   NaN                             Yukon         Canada  64.2823   \n",
       "3599   NaN                       Toronto, ON         Canada      NaN   \n",
       "3601   NaN                      Montreal, QC         Canada      NaN   \n",
       "3602   NaN                 Edmonton, Alberta         Canada      NaN   \n",
       "3603   NaN                  Calgary, Alberta         Canada      NaN   \n",
       "3604   NaN                        London, ON         Canada      NaN   \n",
       "\n",
       "         Long_  2020-03-26  2020-03-25  2020-03-24  2020-03-23  ...  \\\n",
       "3167 -116.5765       486.0       358.0       359.0       301.0  ...   \n",
       "3174 -127.6476       725.0       617.0       617.0       472.0  ...   \n",
       "3179    0.0000         0.0         0.0         0.0         0.0  ...   \n",
       "3187    0.0000        13.0        13.0        13.0        13.0  ...   \n",
       "3209  -98.8139        36.0        35.0        21.0        20.0  ...   \n",
       "3213  -66.4619        33.0        18.0        18.0        17.0  ...   \n",
       "3216  -57.6604        82.0        35.0        35.0        24.0  ...   \n",
       "3220 -124.8457         1.0         NaN         NaN         NaN  ...   \n",
       "3221  -63.7443        73.0        68.0        51.0        41.0  ...   \n",
       "3222  -85.3232       858.0       688.0       588.0       503.0  ...   \n",
       "3223  -63.4168         5.0         5.0         3.0         3.0  ...   \n",
       "3226  -73.5491      1632.0      1342.0      1013.0       628.0  ...   \n",
       "3228    0.0000         0.0         0.0         0.0         NaN  ...   \n",
       "3232 -106.4509        95.0        72.0        72.0        66.0  ...   \n",
       "3248 -135.0000         3.0         NaN         NaN         NaN  ...   \n",
       "3599       NaN         NaN         NaN         NaN         NaN  ...   \n",
       "3601       NaN         NaN         NaN         NaN         NaN  ...   \n",
       "3602       NaN         NaN         NaN         NaN         NaN  ...   \n",
       "3603       NaN         NaN         NaN         NaN         NaN  ...   \n",
       "3604       NaN         NaN         NaN         NaN         NaN  ...   \n",
       "\n",
       "      2020-03-16  2020-03-15  2020-03-14  2020-03-13  2020-03-12  2020-03-11  \\\n",
       "3167        56.0        39.0        29.0        29.0        19.0        19.0   \n",
       "3174       103.0        73.0        64.0        64.0        46.0        39.0   \n",
       "3179         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3187         2.0         NaN         NaN         2.0         NaN         NaN   \n",
       "3209         7.0         4.0         4.0         4.0         NaN         NaN   \n",
       "3213         6.0         2.0         1.0         1.0         1.0         1.0   \n",
       "3216         1.0         1.0         NaN         NaN         NaN         NaN   \n",
       "3220         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3221         5.0         NaN         NaN         NaN         NaN         NaN   \n",
       "3222       177.0       104.0        79.0        74.0        42.0        41.0   \n",
       "3223         1.0         1.0         NaN         NaN         NaN         NaN   \n",
       "3226        50.0        24.0        17.0        17.0         9.0         8.0   \n",
       "3228         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3232         7.0         2.0         2.0         2.0         NaN         NaN   \n",
       "3248         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3599         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3601         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3602         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3603         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3604         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "      2020-03-10  2020-03-09  2020-03-08  2020-03-07  \n",
       "3167         7.0         7.0         NaN         NaN  \n",
       "3174        32.0        32.0        27.0        21.0  \n",
       "3179         NaN         NaN         NaN         NaN  \n",
       "3187         NaN         NaN         NaN         NaN  \n",
       "3209         NaN         NaN         NaN         NaN  \n",
       "3213         NaN         NaN         NaN         NaN  \n",
       "3216         NaN         NaN         NaN         NaN  \n",
       "3220         NaN         NaN         NaN         NaN  \n",
       "3221         NaN         NaN         NaN         NaN  \n",
       "3222        36.0        34.0         NaN         NaN  \n",
       "3223         NaN         NaN         NaN         NaN  \n",
       "3226         4.0         3.0         NaN         NaN  \n",
       "3228         NaN         NaN         NaN         NaN  \n",
       "3232         NaN         NaN         NaN         NaN  \n",
       "3248         NaN         NaN         NaN         NaN  \n",
       "3599         NaN         NaN        28.0        27.0  \n",
       "3601         NaN         NaN         4.0         3.0  \n",
       "3602         NaN         NaN         3.0         1.0  \n",
       "3603         NaN         NaN         1.0         1.0  \n",
       "3604         NaN         NaN         1.0         1.0  \n",
       "\n",
       "[20 rows x 26 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.loc[cdf[\"Country_Region\"]==\"Canada\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_csv(\"./data/confirmed_timeseries.csv\")\n",
    "ddf.to_csv(\"./data/death_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an API for managing the data\n",
    "\n",
    "The API needs to do a few things:\n",
    "\n",
    "1. Look for local cached copy of processed data\n",
    "1. If not present, load data from the beginning of time\n",
    "1. If present, load from the most recent date in cache\n",
    "1. Process data\n",
    "1. Cache results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with loading data and finding most recent date in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "def try_parse(date):\n",
    "    \"\"\"Try and parse a string into a date. Returns a tuple of (True/False, parsed date)\"\"\"\n",
    "    try:\n",
    "        result = dt.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        return (True, result)\n",
    "    except ValueError:\n",
    "        return (False, None)\n",
    "    \n",
    "def get_dates(columns):\n",
    "    \"\"\"Construct a list from an existing list that has date and non-date values in it\"\"\"\n",
    "    if len(columns) == 0:\n",
    "        raise ValueError(\"columns must have length > 0\")\n",
    "        \n",
    "    result = []\n",
    "    for column in columns:\n",
    "        is_date, date = try_parse(column)\n",
    "        if is_date:\n",
    "            result.append(date.date())\n",
    "    return result\n",
    "\n",
    "def find_max(dates):\n",
    "    \"\"\"Find maximum date value in list\"\"\"\n",
    "    if len(dates) == 0:\n",
    "        raise ValueError(\"dates must have length > 0\")\n",
    "        \n",
    "    max = dates[0]\n",
    "    for i in range(1, len(dates)):\n",
    "        if dates[i] > max:\n",
    "            max = dates[i]\n",
    "    return max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the code above ... really need to turn this into a proper test once I figure out how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./data/confirmed_timeseries.csv does not exist: './data/confirmed_timeseries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e06d61a64f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/confirmed_timeseries.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfind_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./data/confirmed_timeseries.csv does not exist: './data/confirmed_timeseries.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/confirmed_timeseries.csv\")\n",
    "find_max(get_dates(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test variation of the algorithm that will start with oldest daily update file and append data until we get to today. First begin with the code that will read the daily report and adjust between new vs. old schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "DAILY_REPORT_BASE_URI = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "\n",
    "state_abbreviations = pd.read_csv(\"./data/state_abbreviations.csv\", index_col=\"Abbreviation\")\n",
    "\n",
    "def extract_state(province_state):\n",
    "    province_state = str(province_state)\n",
    "    if province_state == \"\":\n",
    "        return province_state\n",
    "    res = re.match(\"(.*?)\\s+County,\\s+(\\S*)\", province_state)\n",
    "    if res == None:\n",
    "        res = re.match(\"(.*?),\\s+(\\S*)\", province_state)\n",
    "        if res == None:\n",
    "            return province_state\n",
    "    county, state = res.group(1), res.group(2)\n",
    "    if state in state_abbreviations.index:\n",
    "        return state_abbreviations.loc[state].iloc[0]\n",
    "    else:\n",
    "        return state\n",
    "\n",
    "def extract_county(province_state):\n",
    "    province_state = str(province_state)\n",
    "    if province_state == \"\":\n",
    "        return province_state\n",
    "    res = re.match(\"(.*?)(\\s+County)?,\\s+(\\S*)\", province_state)\n",
    "    if res == None:\n",
    "        res = re.match(\"(.*?),\\s+(\\S*)\", province_state)\n",
    "        if res == None:\n",
    "            return province_state\n",
    "        else:\n",
    "            return res.group(1)\n",
    "    return res.group(1)\n",
    "\n",
    "def read_daily_report(date):\n",
    "    \"\"\"Read the daily report for date\"\"\"\n",
    "    assert(isinstance(date, dt.date)), \"parameter date must be a date object\"\n",
    "    uri = DAILY_REPORT_BASE_URI + date.strftime(\"%m-%d-%Y.csv\")\n",
    "    df = pd.read_csv(uri)\n",
    "    \n",
    "    # Adjust for old schema\n",
    "    if date < dt.date(2020, 3, 22):\n",
    "        df = df.rename(columns={\"Province/State\": \"Province_State\", \"Country/Region\": \"Country_Region\"})\n",
    "        df[\"Admin2\"] = df[\"Province_State\"].apply(extract_county)\n",
    "        df[\"Province_State\"] = df[\"Province_State\"].apply(extract_state)\n",
    "        df[\"FIPS\"] = 0\n",
    "        df[\"Lat\"] = 0.0\n",
    "        df[\"Long_\"] = 0.0\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add the function that will read each historical daily file and append to a list containing tuples of (date, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import urllib\n",
    "\n",
    "EARLIEST_DATE = dt.date(2020, 1, 22)\n",
    "\n",
    "def read_historical_data(start_date, end_date):\n",
    "    \"\"\"Read all of the historical data starting at start_date\"\"\"\n",
    "    def daterange(start, end):\n",
    "        for day in range(int((end-start).days)+1):\n",
    "            yield start + dt.timedelta(day)\n",
    "    \n",
    "    result = []\n",
    "    for day in daterange(start_date, end_date):\n",
    "        try:\n",
    "            df = read_daily_report(day)\n",
    "            result.append((day, df))\n",
    "        except urllib.error.HTTPError as err:\n",
    "            # Today's data may not be available\n",
    "            if err.code == 404 and day == dt.date.today():\n",
    "                return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_data = read_historical_data(EARLIEST_DATE, dt.date(2020, 3, 27))\n",
    "len(historical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try running the algorithm from the oldest date to the newest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized = False\n",
    "for d, df in historical_data:\n",
    "    if not initialized:\n",
    "        cdf = df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Confirmed\"]].fillna(value={\"Admin2\": \"\"}, downcast=\"infer\")\n",
    "        ddf = df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Deaths\"]].fillna(value={\"Admin2\": \"\"}, downcast=\"infer\")\n",
    "        initialized = True\n",
    "    else:\n",
    "        df = df.fillna(value={\"Admin2\": \"\", \"Province_State\": \"\"}, downcast=\"infer\")\n",
    "        cdf = cdf.merge(df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Confirmed\", \"Lat\", \"Long_\"]], on=[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\"], how=\"outer\")\n",
    "        ddf = ddf.merge(df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Deaths\", \"Lat\", \"Long_\"]], on=[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\"], how=\"outer\")\n",
    "\n",
    "    # Rename the merged columns\n",
    "    cdf = cdf.rename(columns={\"Confirmed\": d})\n",
    "    ddf = ddf.rename(columns={\"Deaths\": d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the time series to disk so I can inspect it using Excel you know I meant Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.tail(10)\n",
    "cdf.to_csv(\"./data/confirmed_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing everything together into a single function that initializes the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "DAILY_REPORT_BASE_URI = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "CACHED_CONFIRMED = \"./data/confirmed_timeseries.csv\"\n",
    "CACHED_DEATHS = \"./data/death_timeseries.csv\"\n",
    "EARLIEST_DATE = dt.date(2020, 1, 22)\n",
    "\n",
    "confirmed_df = None\n",
    "deaths_df = None\n",
    "\n",
    "def init(most_recent_date):\n",
    "    \"\"\"Initialize API from most_recent_date to earliest known date while taking advantage of local caching\"\"\"\n",
    "    \n",
    "    global confirmed_df, deaths_df\n",
    "    if most_recent_date == None:\n",
    "        most_recent_date = dt.date.today()\n",
    "    \n",
    "    # I'm not in the mood to handle partial failures\n",
    "    cache_ok = False\n",
    "    start_date = EARLIEST_DATE\n",
    "    if os.path.isfile(CACHED_CONFIRMED) and os.path.isfile(CACHED_DEATHS):\n",
    "        confirmed_df = pd.read_csv(CACHED_CONFIRMED)\n",
    "        deaths_df = pd.read_csv(CACHED_DEATHS)\n",
    "\n",
    "        # Find the most recent date based on column labels\n",
    "        confirmed_most_recent_date = find_max(get_dates(confirmed_df))\n",
    "        deaths_most_recent_date = find_max(get_dates(deaths_df))\n",
    "\n",
    "        if confirmed_most_recent_date == deaths_most_recent_date:\n",
    "            cache_ok = True\n",
    "            start_date = confirmed_most_recent_date\n",
    "\n",
    "    # Read historical data from start_date to today\n",
    "    historical_data = read_historical_data(start_date, most_recent_date)\n",
    "    \n",
    "    initialized = False\n",
    "    for d, df in historical_data:\n",
    "        if not initialized:\n",
    "            if not cache_ok:\n",
    "                confirmed_df = df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Confirmed\"]].fillna(value={\"Admin2\": \"\"}, downcast=\"infer\")\n",
    "                deaths_df = df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Deaths\"]].fillna(value={\"Admin2\": \"\"}, downcast=\"infer\")\n",
    "            initialized = True\n",
    "        else:\n",
    "            df = df.fillna(value={\"Admin2\": \"\", \"Province_State\": \"\"}, downcast=\"infer\")\n",
    "            confirmed_df = confirmed_df.merge(df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Confirmed\"]], on=[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\"], how=\"outer\")\n",
    "            deaths_df = deaths_df.merge(df[[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\", \"Deaths\"]], on=[\"FIPS\", \"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\"], how=\"outer\")\n",
    "\n",
    "        # Rename the merged columns\n",
    "        confirmed_df = confirmed_df.rename(columns={\"Confirmed\": str(d)})\n",
    "        deaths_df = deaths_df.rename(columns={\"Deaths\": str(d)})\n",
    "\n",
    "    # Write back out to cache\n",
    "    confirmed_df.to_csv(CACHED_CONFIRMED, index=False)\n",
    "    deaths_df.to_csv(CACHED_DEATHS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(dt.date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's write some APIs that will retrieve the data from the dataset\n",
    "\n",
    "`get_data(country, region, sub_region)`. If given just `country`, the API will return the aggregated time series for the entire country. If given `country, region` it will return the aggregated time series for all of the sub regions within `country, region`. If given all three, `country, region, sub_region` it will return the time series for that tuple. I think what we need to do is use the pandas [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs will not return aggregated data, but instead will return just the dataset based on the query and its up to you to `sum()` over rows if that's what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [str(d) for d in get_dates(confirmed_df.columns)]\n",
    "res = confirmed_df.loc[(confirmed_df[\"Country_Region\"]==\"US\") & (confirmed_df[\"Province_State\"]==\"Washington\") & (confirmed_df[\"Admin2\"] == \"King\"), dates].sum()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate APIs:\n",
    "\n",
    "Concepts: `country`, `state`, `region`\n",
    "\n",
    "- `get_all_countries()` returns aggregated data for each country\n",
    "- `get_all_regions_within_state(country, state)` if state is None then returns all regions for that country. if state has a value, returns all regions for that state.\n",
    "- `get_region(country, state, region)` returns a time series for just that region (or should it be a dataframe with just that row ...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the aggregated data for all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.groupby([\"Country_Region\"]).sum()[[\"Lat\", \"Long_\"] + dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return all regions within the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.loc[(confirmed_df[\"Country_Region\"]==\"US\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return all regions in US grouped by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.loc[(confirmed_df[\"Country_Region\"]==\"US\")].groupby([\"Province_State\"]).sum()[[\"Lat\", \"Long_\"] + dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return all regions within Washington State in US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.loc[(confirmed_df[\"Country_Region\"] == \"US\") & (confirmed_df[\"Province_State\"] == \"Washington\")][[\"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\"]+dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return specific region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.loc[(confirmed_df[\"Country_Region\"] == \"US\") & (confirmed_df[\"Province_State\"] == \"Washington\") & (confirmed_df[\"Admin2\"] == \"King\")][[\"Admin2\", \"Province_State\", \"Country_Region\", \"Lat\", \"Long_\"]+dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the metadata for each `Country_Region`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.drop_duplicates(\"Country_Region\")[[\"Country_Region\", \"Lat\", \"Long_\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of all of the `Province_State` within a `Country_Region`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df.loc[confirmed_df[\"Country_Region\"]==\"US\"][\"Province_State\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an API that will retrieve either confirmed cases or deaths given `country`, `region`, `sub_region` as parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_date_column_names(df):\n",
    "    return [str(d) for d in get_dates(df.columns)]\n",
    "\n",
    "def get_countries():\n",
    "    columns = [\"Lat\", \"Long_\"] + get_date_column_names(confirmed_df)\n",
    "    return (confirmed_df.groupby([\"Country_Region\"]).sum()[columns],\n",
    "            deaths_df.groupby([\"Country_Region\"]).sum()[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_countries()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_country_states(country):\n",
    "    columns = [\"FIPS\", \"Lat\", \"Long_\"] + get_date_column_names(confirmed_df)\n",
    "    return (confirmed_df.loc[confirmed_df[\"Country_Region\"] == country].groupby([\"Province_State\", \"Country_Region\"]).sum()[columns],\n",
    "            deaths_df.loc[deaths_df[\"Country_Region\"] == country].groupby([\"Province_State\", \"Country_Region\"]).sum()[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_states(\"US\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_country_state_regions(country, state):\n",
    "    columns = [\"Country_Region\", \"Province_State\", \"Admin2\", \"FIPS\", \"Lat\", \"Long_\"] + get_date_column_names(confirmed_df)\n",
    "    return (confirmed_df.loc[(confirmed_df[\"Country_Region\"] == country) & (confirmed_df[\"Province_State\"] == state)][columns],\n",
    "            deaths_df.loc[(deaths_df[\"Country_Region\"] == country) & (deaths_df[\"Province_State\"] == state)][columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_date_column_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-10bd2e0ef23c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_country_state_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"US\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"New York\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-d44c058068e8>\u001b[0m in \u001b[0;36mget_country_state_regions\u001b[0;34m(country, state)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_country_state_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Country_Region\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Province_State\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Admin2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FIPS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Lat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Long_\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_date_column_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfirmed_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     return (confirmed_df.loc[(confirmed_df[\"Country_Region\"] == country) & (confirmed_df[\"Province_State\"] == state)][columns],\n\u001b[1;32m      6\u001b[0m             deaths_df.loc[(deaths_df[\"Country_Region\"] == country) & (deaths_df[\"Province_State\"] == state)][columns])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_date_column_names' is not defined"
     ]
    }
   ],
   "source": [
    "get_country_state_regions(\"US\", \"New York\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some APIs for getting names of countries, states, and regions for interactive applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_country_names():\n",
    "    return confirmed_df[\"Country_Region\"].unique()\n",
    "\n",
    "def get_state_names(country):\n",
    "    return confirmed_df.loc[confirmed_df[\"Country_Region\"]==country][\"Province_State\"].unique()\n",
    "\n",
    "def get_region_names(country, state):\n",
    "    return confirmed_df.loc[(confirmed_df[\"Country_Region\"]==country)&(confirmed_df[\"Province_State\"]==state)][\"Admin2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_region_names(\"US\", \"Washington\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
