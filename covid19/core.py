# AUTOGENERATED! DO NOT EDIT! File to edit: 01_data.ipynb (unless otherwise specified).

__all__ = ['read_daily_report', 'DAILY_REPORT_BASE_URI', 'read_daily_report', 'DAILY_REPORT_BASE_URI', 'try_parse',
           'get_dates', 'find_max', 'extract_state', 'extract_county', 'read_daily_report', 'DAILY_REPORT_BASE_URI',
           'state_abbreviations', 'read_historical_data', 'EARLIEST_DATE', 'init', 'DAILY_REPORT_BASE_URI',
           'CACHED_CONFIRMED', 'CACHED_DEATHS', 'EARLIEST_DATE', 'confirmed_df', 'deaths_df', 'get_date_column_names',
           'get_countries', 'get_country_states', 'get_country_state_regions', 'get_country_names', 'get_state_names',
           'get_region_names']

# Cell
import datetime as dt

DAILY_REPORT_BASE_URI = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/"

def read_daily_report(date):
    """Read the daily report for date"""
    assert(isinstance(date, dt.date)), "parameter date must be a date object"
    uri = DAILY_REPORT_BASE_URI + date.strftime("%m-%d-%Y.csv")
    df = pd.read_csv(uri)
    df = df.rename(columns={"Province/State": "Province_State", "Country/Region": "Country_Region"})
    df.set_index(['Province_State', 'Country_Region'])
    return df

# Cell
import pandas as pd
import datetime as dt

DAILY_REPORT_BASE_URI = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/"

def read_daily_report(date):
    """Read the daily report for date"""
    assert(isinstance(date, dt.date)), "parameter date must be a date object"
    uri = DAILY_REPORT_BASE_URI + date.strftime("%m-%d-%Y.csv")
    df = pd.read_csv(uri)

    # Adjust for old schema
    if date < dt.date(2020, 3, 22):
        df = df.rename(columns={"Province/State": "Province_State", "Country/Region": "Country_Region"})
        df["Admin2"] = df["Province_State"].apply(extract_county)
        df["Province_State"] = df["Province_State"].apply(extract_state)

    return df

# Cell

import pandas as pd
import datetime as dt

def try_parse(date):
    """Try and parse a string into a date. Returns a tuple of (True/False, parsed date)"""
    try:
        result = dt.datetime.strptime(date, "%Y-%m-%d")
        return (True, result)
    except ValueError:
        return (False, None)

def get_dates(columns):
    """Construct a list from an existing list that has date and non-date values in it"""
    if len(columns) == 0:
        raise ValueError("columns must have length > 0")

    result = []
    for column in columns:
        is_date, date = try_parse(column)
        if is_date:
            result.append(date.date())
    return result

def find_max(dates):
    """Find maximum date value in list"""
    if len(dates) == 0:
        raise ValueError("dates must have length > 0")

    max = dates[0]
    for i in range(1, len(dates)):
        if dates[i] > max:
            max = dates[i]
    return max

# Cell
import re
import datetime as dt

DAILY_REPORT_BASE_URI = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/"

state_abbreviations = pd.read_csv("./data/state_abbreviations.csv", index_col="Abbreviation")

def extract_state(province_state):
    province_state = str(province_state)
    if province_state == "":
        return province_state
    res = re.match("(.*?)\s+County,\s+(\S*)", province_state)
    if res == None:
        res = re.match("(.*?),\s+(\S*)", province_state)
        if res == None:
            return province_state
    county, state = res.group(1), res.group(2)
    if state in state_abbreviations.index:
        return state_abbreviations.loc[state].iloc[0]
    else:
        return state

def extract_county(province_state):
    province_state = str(province_state)
    if province_state == "":
        return province_state
    res = re.match("(.*?)(\s+County)?,\s+(\S*)", province_state)
    if res == None:
        res = re.match("(.*?),\s+(\S*)", province_state)
        if res == None:
            return province_state
        else:
            return res.group(1)
    return res.group(1)

def read_daily_report(date):
    """Read the daily report for date"""
    assert(isinstance(date, dt.date)), "parameter date must be a date object"
    uri = DAILY_REPORT_BASE_URI + date.strftime("%m-%d-%Y.csv")
    df = pd.read_csv(uri)

    # Adjust for old schema
    if date < dt.date(2020, 3, 22):
        df = df.rename(columns={"Province/State": "Province_State", "Country/Region": "Country_Region"})
        df["Admin2"] = df["Province_State"].apply(extract_county)
        df["Province_State"] = df["Province_State"].apply(extract_state)
        df["FIPS"] = 0
        df["Lat"] = 0.0
        df["Long_"] = 0.0

    return df

# Cell
import urllib

EARLIEST_DATE = dt.date(2020, 1, 22)

def read_historical_data(start_date, end_date):
    """Read all of the historical data starting at start_date"""
    def daterange(start, end):
        for day in range(int((end-start).days)+1):
            yield start + dt.timedelta(day)

    result = []
    for day in daterange(start_date, end_date):
        try:
            df = read_daily_report(day)
            result.append((day, df))
        except urllib.error.HTTPError as err:
            # Today's data may not be available
            if err.code == 404 and day == dt.date.today():
                return result
    return result

# Cell

import os
import datetime as dt
import pandas as pd

DAILY_REPORT_BASE_URI = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/"
CACHED_CONFIRMED = "./data/confirmed_timeseries.csv"
CACHED_DEATHS = "./data/death_timeseries.csv"
EARLIEST_DATE = dt.date(2020, 1, 22)

confirmed_df = None
deaths_df = None

def init(most_recent_date):
    """Initialize API from most_recent_date to earliest known date while taking advantage of local caching"""

    global confirmed_df, deaths_df
    if most_recent_date == None:
        most_recent_date = dt.date.today()

    # I'm not in the mood to handle partial failures
    cache_ok = False
    start_date = EARLIEST_DATE
    if os.path.isfile(CACHED_CONFIRMED) and os.path.isfile(CACHED_DEATHS):
        confirmed_df = pd.read_csv(CACHED_CONFIRMED)
        deaths_df = pd.read_csv(CACHED_DEATHS)

        # Find the most recent date based on column labels
        confirmed_most_recent_date = find_max(get_dates(confirmed_df))
        deaths_most_recent_date = find_max(get_dates(deaths_df))

        if confirmed_most_recent_date == deaths_most_recent_date:
            cache_ok = True
            start_date = confirmed_most_recent_date

    # Read historical data from start_date to today
    historical_data = read_historical_data(start_date, most_recent_date)

    initialized = False
    for d, df in historical_data:
        if not initialized:
            if not cache_ok:
                confirmed_df = df[["FIPS", "Admin2", "Province_State", "Country_Region", "Lat", "Long_", "Confirmed"]].fillna(value={"Admin2": ""}, downcast="infer")
                deaths_df = df[["FIPS", "Admin2", "Province_State", "Country_Region", "Lat", "Long_", "Deaths"]].fillna(value={"Admin2": ""}, downcast="infer")
            initialized = True
        else:
            df = df.fillna(value={"Admin2": "", "Province_State": ""}, downcast="infer")
            confirmed_df = confirmed_df.merge(df[["FIPS", "Admin2", "Province_State", "Country_Region", "Lat", "Long_", "Confirmed"]], on=["FIPS", "Admin2", "Province_State", "Country_Region", "Lat", "Long_"], how="outer")
            deaths_df = deaths_df.merge(df[["FIPS", "Admin2", "Province_State", "Country_Region", "Lat", "Long_", "Deaths"]], on=["FIPS", "Admin2", "Province_State", "Country_Region", "Lat", "Long_"], how="outer")

        # Rename the merged columns
        confirmed_df = confirmed_df.rename(columns={"Confirmed": str(d)})
        deaths_df = deaths_df.rename(columns={"Deaths": str(d)})

    # Write back out to cache
    confirmed_df.to_csv(CACHED_CONFIRMED, index=False)
    deaths_df.to_csv(CACHED_DEATHS, index=False)

# Cell

def get_date_column_names(df):
    return [str(d) for d in get_dates(df.columns)]

def get_countries():
    columns = ["Lat", "Long_"] + get_date_column_names(confirmed_df)
    return (confirmed_df.groupby(["Country_Region"]).sum()[columns],
            deaths_df.groupby(["Country_Region"]).sum()[columns])

# Cell

def get_country_states(country):
    columns = ["FIPS", "Lat", "Long_"] + get_date_column_names(confirmed_df)
    return (confirmed_df.loc[confirmed_df["Country_Region"] == country].groupby(["Province_State", "Country_Region"]).sum()[columns],
            deaths_df.loc[deaths_df["Country_Region"] == country].groupby(["Province_State", "Country_Region"]).sum()[columns])

# Cell

def get_country_state_regions(country, state):
    columns = ["Country_Region", "Province_State", "Admin2", "FIPS", "Lat", "Long_"] + get_date_column_names(confirmed_df)
    return (confirmed_df.loc[(confirmed_df["Country_Region"] == country) & (confirmed_df["Province_State"] == state)][columns],
            deaths_df.loc[(deaths_df["Country_Region"] == country) & (deaths_df["Province_State"] == state)][columns])

# Cell

def get_country_names():
    return confirmed_df["Country_Region"].unique()

def get_state_names(country):
    return confirmed_df.loc[confirmed_df["Country_Region"]==country]["Province_State"].unique()

def get_region_names(country, state):
    return confirmed_df.loc[(confirmed_df["Country_Region"]==country)&(confirmed_df["Province_State"]==state)]["Admin2"].unique()